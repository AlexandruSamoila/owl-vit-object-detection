{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a quick experiment to test the assumption that embeddings straight out of an untrained Owl-VIT model are indeed good for zero-shot classification.\n",
    "\n",
    "The assumption is that the Owl-VIT model out of the box produces meaningful embeddings for each object detected, and embeds them in latent space such that there is some \n",
    "meaningful distance between different objects. The test for this is as follows:\n",
    "\n",
    "1. Pick an image containing *one* object on a blank background.\n",
    "2. Gather embeddings, dim reduce and visiualize - we should see two clearly seperable clusters, one that represents the object embeddings and one that represents the background noise embeddings (there may be more than one \"background\" cluster since I'm not sure how Owl handles noise embeddings)\n",
    "3. Use k-means with k=2 to classify each point in an unsupervised manner\n",
    "4. Overlay bounding boxes on the image for each cluster\n",
    "\n",
    "What we should see then from each image are bounding boxes around the object of interest for the non-noise cluster's boxes, and bounding boxes scattered about the image chaotically for the background cluster's boxes.\n",
    "\n",
    "**Result:** As expected. Boxes cluster where you'd expect hinting that the embeddings are useful right out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steve/miniconda3/envs/owl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/steve/miniconda3/envs/owl/lib/python3.10/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/steve/miniconda3/envs/owl/lib/python3.10/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/steve/miniconda3/envs/owl/lib/python3.10/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/steve/miniconda3/envs/owl/lib/python3.10/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "OwlViT.__init__() got an unexpected keyword argument 'num_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m w, h \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39msize \n\u001b[1;32m     20\u001b[0m image_processor \u001b[39m=\u001b[39m OwlViTProcessor\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mgoogle/owlvit-base-patch32\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m model \u001b[39m=\u001b[39m OwlViT(num_classes\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)  \u001b[39m# no classes since we're not using the classifier, just the image embedder\u001b[39;00m\n\u001b[1;32m     22\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     23\u001b[0m post \u001b[39m=\u001b[39m PostProcess(confidence_threshold\u001b[39m=\u001b[39m\u001b[39m0.0\u001b[39m, iou_threshold\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m)  \u001b[39m# keep all boxes\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: OwlViT.__init__() got an unexpected keyword argument 'num_classes'"
     ]
    }
   ],
   "source": [
    "import notebook_helper\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from transformers import OwlViTProcessor\n",
    "import torch\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from src.models import OwlViT, PostProcess\n",
    "from src.util import BoxUtil\n",
    "from src.main import model_output_to_image\n",
    "\n",
    "n_kmeans_clusters = 2\n",
    "impath = \"assets/dog-on-white.jpg\"\n",
    "image = Image.open(impath)\n",
    "w, h = image.size \n",
    "\n",
    "image_processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "model = OwlViT(num_classes=0)  # no classes since we're not using the classifier, just the image embedder\n",
    "model.eval()\n",
    "post = PostProcess(confidence_threshold=0.0, iou_threshold=1.0)  # keep all boxes\n",
    "\n",
    "image = image_processor(images=image, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "with torch.no_grad():\n",
    "    pred_boxes, embeddings = model(image, return_with_embeddings=True)\n",
    "    pred_boxes = model_output_to_image(pred_boxes, {\"width\": w, \"height\": h})\n",
    "    embeddings = embeddings.squeeze(0).numpy()\n",
    "\n",
    "reduced = notebook_helper.get_reduced(embeddings, 3)\n",
    "kmeans = KMeans(n_clusters=n_kmeans_clusters, random_state=0, n_init=\"auto\").fit(reduced)\n",
    "labels = torch.tensor(kmeans.labels_).unsqueeze(0)\n",
    "print(kmeans.labels_)\n",
    "fig = notebook_helper.make_plot_3d(reduced, colors=kmeans.labels_)\n",
    "display(fig)\n",
    "\n",
    "for label in range(n_kmeans_clusters):\n",
    "    _pred_boxes = pred_boxes[torch.where(labels == label)].unsqueeze(0)\n",
    "    image_with_boxes = BoxUtil.draw_box_on_image(impath, _pred_boxes)\n",
    "    plt.imshow(image_with_boxes.squeeze(0).permute(1,2,0).numpy(), interpolation='nearest')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "owl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
