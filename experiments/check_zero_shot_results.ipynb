{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a quick experiment to test the assumption that embeddings straight out of an untrained Owl-VIT model are indeed good for zero-shot classification.\n",
    "\n",
    "The assumption is that the Owl-VIT model out of the box produces meaningful embeddings for each object detected, and embeds them in latent space such that there is some \n",
    "meaningful distance between different objects. The test for this is as follows:\n",
    "\n",
    "1. Pick an image containing *one* object on a blank background.\n",
    "2. Gather embeddings, dim reduce and visiualize - we should see two clearly seperable clusters, one that represents the object embeddings and one that represents the background noise embeddings (there may be more than one \"background\" cluster since I'm not sure how Owl handles noise embeddings)\n",
    "3. Use k-means with k=2 to classify each point in an unsupervised manner\n",
    "4. Overlay bounding boxes on the image for each cluster\n",
    "\n",
    "What we should see then from each image are bounding boxes around the object of interest for the non-noise cluster's boxes, and bounding boxes scattered about the image chaotically for the background cluster's boxes.\n",
    "\n",
    "**Result:** As expected. Boxes cluster where you'd expect hinting that the embeddings are useful right out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from transformers import OwlViTProcessor\n",
    "import torch\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from models import OwlViT, PostProcess\n",
    "from util import BoxUtil\n",
    "from main import model_output_to_image\n",
    "import notebook_helper\n",
    "\n",
    "n_kmeans_clusters = 2\n",
    "impath = \"assets/dog-on-white.jpg\"\n",
    "image = Image.open(impath)\n",
    "w, h = image.size \n",
    "\n",
    "image_processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "model = OwlViT(num_classes=0)  # no classes since we're not using the classifier, just the image embedder\n",
    "model.eval()\n",
    "post = PostProcess(confidence_threshold=0.0, iou_threshold=1.0)  # keep all boxes\n",
    "\n",
    "image = image_processor(images=image, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "with torch.no_grad():\n",
    "    pred_boxes, _, embeddings = model(image, return_with_embeddings=True)\n",
    "    pred_boxes = model_output_to_image(pred_boxes, {\"width\": w, \"height\": h})\n",
    "    embeddings = embeddings.squeeze(0).numpy()\n",
    "\n",
    "reduced = notebook_helper.get_reduced(embeddings, 3)\n",
    "kmeans = KMeans(n_clusters=n_kmeans_clusters, random_state=0, n_init=\"auto\").fit(reduced)\n",
    "labels = torch.tensor(kmeans.labels_).unsqueeze(0)\n",
    "print(kmeans.labels_)\n",
    "fig = notebook_helper.make_plot_3d(reduced, colors=kmeans.labels_)\n",
    "display(fig)\n",
    "\n",
    "for label in range(n_kmeans_clusters):\n",
    "    _pred_boxes = pred_boxes[torch.where(labels == label)].unsqueeze(0)\n",
    "    image_with_boxes = BoxUtil.draw_box_on_image(impath, _pred_boxes)\n",
    "    plt.imshow(image_with_boxes.squeeze(0).permute(1,2,0).numpy(), interpolation='nearest')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "owl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
